{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVfWZsUFZKma"
   },
   "source": [
    "The examples in this notebook is mostly from the DeepLearning.AI's short course on LangChain.\n",
    "\n",
    "Please check out the course for more information:\n",
    "https://learn.deeplearning.ai/langchain/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zi03CqoXfrdI"
   },
   "source": [
    "# Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "!echo $OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                   Version\n",
      "------------------------- ---------------\n",
      "aiohappyeyeballs          2.4.3\n",
      "aiohttp                   3.10.9\n",
      "aiosignal                 1.3.1\n",
      "annotated-types           0.7.0\n",
      "anyio                     4.6.0\n",
      "appnope                   0.1.4\n",
      "argon2-cffi               23.1.0\n",
      "argon2-cffi-bindings      21.2.0\n",
      "arrow                     1.3.0\n",
      "asttokens                 2.4.1\n",
      "async-lru                 2.0.4\n",
      "attrs                     24.2.0\n",
      "babel                     2.16.0\n",
      "beautifulsoup4            4.12.3\n",
      "bleach                    6.1.0\n",
      "certifi                   2024.8.30\n",
      "cffi                      1.17.1\n",
      "charset-normalizer        3.3.2\n",
      "click                     8.1.7\n",
      "comm                      0.2.2\n",
      "dataclasses-json          0.6.7\n",
      "DateTime                  5.5\n",
      "debugpy                   1.8.6\n",
      "decorator                 5.1.1\n",
      "defusedxml                0.7.1\n",
      "distro                    1.9.0\n",
      "docarray                  0.40.0\n",
      "duckduckgo_search         6.3.0\n",
      "executing                 2.1.0\n",
      "fastjsonschema            2.20.0\n",
      "fqdn                      1.5.1\n",
      "frozenlist                1.4.1\n",
      "greenlet                  3.1.1\n",
      "h11                       0.14.0\n",
      "httpcore                  1.0.6\n",
      "httpx                     0.27.2\n",
      "idna                      3.10\n",
      "ipykernel                 6.29.5\n",
      "ipython                   8.28.0\n",
      "isoduration               20.11.0\n",
      "jedi                      0.19.1\n",
      "Jinja2                    3.1.4\n",
      "jiter                     0.5.0\n",
      "json5                     0.9.25\n",
      "jsonpatch                 1.33\n",
      "jsonpointer               3.0.0\n",
      "jsonschema                4.23.0\n",
      "jsonschema-specifications 2023.12.1\n",
      "jupyter_client            8.6.3\n",
      "jupyter_core              5.7.2\n",
      "jupyter-events            0.10.0\n",
      "jupyter-lsp               2.2.5\n",
      "jupyter_server            2.14.2\n",
      "jupyter_server_terminals  0.5.3\n",
      "jupyterlab                4.2.5\n",
      "jupyterlab_pygments       0.3.0\n",
      "jupyterlab_server         2.27.3\n",
      "langchain                 0.3.2\n",
      "langchain-community       0.3.1\n",
      "langchain-core            0.3.9\n",
      "langchain-text-splitters  0.3.0\n",
      "langsmith                 0.1.131\n",
      "markdown-it-py            3.0.0\n",
      "MarkupSafe                2.1.5\n",
      "marshmallow               3.22.0\n",
      "matplotlib-inline         0.1.7\n",
      "mdurl                     0.1.2\n",
      "mistune                   3.0.2\n",
      "multidict                 6.1.0\n",
      "mypy-extensions           1.0.0\n",
      "nbclient                  0.10.0\n",
      "nbconvert                 7.16.4\n",
      "nbformat                  5.10.4\n",
      "nest-asyncio              1.6.0\n",
      "notebook_shim             0.2.4\n",
      "numpy                     1.26.4\n",
      "openai                    1.51.0\n",
      "orjson                    3.10.7\n",
      "overrides                 7.7.0\n",
      "packaging                 24.1\n",
      "pandocfilters             1.5.1\n",
      "parso                     0.8.4\n",
      "pexpect                   4.9.0\n",
      "pip                       24.2\n",
      "platformdirs              4.3.6\n",
      "primp                     0.6.3\n",
      "prometheus_client         0.21.0\n",
      "prompt_toolkit            3.0.48\n",
      "psutil                    6.0.0\n",
      "ptyprocess                0.7.0\n",
      "pure_eval                 0.2.3\n",
      "pycparser                 2.22\n",
      "pydantic                  2.9.2\n",
      "pydantic_core             2.23.4\n",
      "pydantic-settings         2.5.2\n",
      "Pygments                  2.18.0\n",
      "python-dateutil           2.9.0.post0\n",
      "python-dotenv             1.0.1\n",
      "python-json-logger        2.0.7\n",
      "pytz                      2024.2\n",
      "PyYAML                    6.0.2\n",
      "pyzmq                     26.2.0\n",
      "referencing               0.35.1\n",
      "regex                     2024.9.11\n",
      "requests                  2.32.3\n",
      "requests-toolbelt         1.0.0\n",
      "rfc3339-validator         0.1.4\n",
      "rfc3986-validator         0.1.1\n",
      "rich                      13.9.2\n",
      "rpds-py                   0.20.0\n",
      "Send2Trash                1.8.3\n",
      "setuptools                75.1.0\n",
      "six                       1.16.0\n",
      "sniffio                   1.3.1\n",
      "soupsieve                 2.6\n",
      "SQLAlchemy                2.0.35\n",
      "stack-data                0.6.3\n",
      "tenacity                  8.5.0\n",
      "terminado                 0.18.1\n",
      "tiktoken                  0.8.0\n",
      "tinycss2                  1.3.0\n",
      "tornado                   6.4.1\n",
      "tqdm                      4.66.5\n",
      "traitlets                 5.14.3\n",
      "types-python-dateutil     2.9.0.20241003\n",
      "types-requests            2.32.0.20240914\n",
      "typing_extensions         4.12.2\n",
      "typing-inspect            0.9.0\n",
      "uri-template              1.3.0\n",
      "urllib3                   2.2.3\n",
      "wcwidth                   0.2.13\n",
      "webcolors                 24.8.0\n",
      "webencodings              0.5.1\n",
      "websocket-client          1.8.0\n",
      "wikipedia                 1.4.0\n",
      "yarl                      1.13.1\n",
      "zope.interface            7.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70ef8vcIb5DD",
    "outputId": "1b2e74b2-85c2-48b8-c20a-0084925740ac"
   },
   "outputs": [],
   "source": [
    "# !pip install python-dotenv\n",
    "# !pip install openai\n",
    "# !pip install --upgrade langchain\n",
    "# !pip install docarray\n",
    "# !pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDPPy00ueaXY"
   },
   "source": [
    "**How do I set the OpenAI API key as an environmental variable?**\n",
    "- Open System properties and select Advanced system settings\n",
    "- Select Environment Variables\n",
    "- Select New and add your name/key value pair\n",
    "-- Variable name: OPENAI_API_KEY\n",
    "-- Variable value: sk-xxxxxxxxxxxxxxxxxxxx\n",
    "- Restart your computer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "k0C_X3K8cOAZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "# If you set the enviorment variable using the steps above, then you can use the following commented out code\n",
    "import os\n",
    "import openai\n",
    "#from dotenv import load_dotenv, find_dotenv\n",
    "#_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "CURRENT_OPENAI_API_KEY = openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "\n",
    "# os.environ['OPENAI_API_KEY'] = CURRENT_OPENAI_API_KEY\n",
    "# print(os.getenv('OPENAI_API_KEY'))\n",
    "# You can use the following on a terminal:\n",
    "# export OPENAI_API_KEY = CURRENT_OPENAI_API_KEY\n",
    "\n",
    "# openai.api_key = CURRENT_OPENAI_API_KEY\n",
    "print(os.environ['OPENAI_API_KEY'] ) \n",
    "\n",
    "# When calling a openai function/class, pass the key value to the parameter as follows:\n",
    "# openai_api_key=CURRENT_OPENAI_API_KEY\n",
    "# for example: chat = ChatOpenAI(openai_api_key=CURRENT_OPENAI_API_KEY, temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7NSZkWPcC_v"
   },
   "source": [
    "# LangChain: Models, Prompts and Output Parsers\n",
    "\n",
    "#### Outline\n",
    "- Direct API calls to OpenAI\n",
    "- API calls through LangChain\n",
    "- Prompts\n",
    "- Models\n",
    "- Output parsers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cr0mlnI7f7cy"
   },
   "source": [
    "### Direct API calls to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "pletdPo6cmz9"
   },
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "# chat_completion = client.chat.completions.create(\n",
    "#     messages=[\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": \"Say this is a test\",\n",
    "#         }\n",
    "#     ],\n",
    "#     model=\"gpt-3.5-turbo\",\n",
    "# )\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    # return response.choices[0].message[\"content\"]\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "zi6rkBXUcocO",
    "outputId": "15665872-09f6-4102-ab23-bbaa44bc937f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1+1 equals 2.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iDcHRA_j3yYO"
   },
   "outputs": [],
   "source": [
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse,\\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "R5GPBMpZ30VV"
   },
   "outputs": [],
   "source": [
    "style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BjgkV0i2318M",
    "outputId": "43acff89-00ee-4e3f-dac9-5fd90f94293a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks\n",
      "into a style that is American English in a calm and respectful tone\n",
      ".\n",
      "text: ```\n",
      "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse,the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"Translate the text \\\n",
    "that is delimited by triple backticks\n",
    "into a style that is {style}.\n",
    "text: ```{customer_email}```\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "XR14ogFX4BGY"
   },
   "outputs": [],
   "source": [
    "response = get_completion(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GeXZloyu4Cfc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I am really frustrated that my blender lid flew off and splattered my kitchen walls with smoothie! And to make matters worse, the warranty doesn't cover the cost of cleaning up my kitchen. I could really use your help right now, friend.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVOWe_PT4gpZ"
   },
   "source": [
    "## API calls through LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fcI8TKWk4pKt"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sv1Tr05I4ubD"
   },
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "39iuOP5t4vr6",
    "outputId": "d556aeec-8360-4f6a-d750-e74a1fb532ca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mh/dw465j0x4b9gkjqfrkgzkxlh0000gn/T/ipykernel_29534/340801409.py:3: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  chat = ChatOpenAI(openai_api_key=CURRENT_OPENAI_API_KEY, temperature=0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x11355e900>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x1135b8530>, temperature=0.0, model_kwargs={}, openai_api_key='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', openai_proxy='')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To control the randomness and creativity of the generated\n",
    "# text by an LLM, use temperature = 0.0\n",
    "chat = ChatOpenAI(openai_api_key=CURRENT_OPENAI_API_KEY, temperature=0.0)\n",
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p89fe6R48m4Y"
   },
   "source": [
    "### Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "uvXKmT_J5joQ"
   },
   "outputs": [],
   "source": [
    "template_string = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "WELIfB6R5g9b"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VhNltUIz5puf",
    "outputId": "50f65786-2b32-4451-e32e-6591a96656f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['style', 'text'], input_types={}, partial_variables={}, template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q8eoiqFT5wk8",
    "outputId": "b3a2ee7f-bac0-4d97-a6a5-65c92e33722a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['style', 'text']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "1apnQP3L5zOl"
   },
   "outputs": [],
   "source": [
    "customer_style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "EupgoHq853AG"
   },
   "outputs": [],
   "source": [
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "uIQHb15E58uj"
   },
   "outputs": [],
   "source": [
    "customer_messages = prompt_template.format_messages(\n",
    "                    style=customer_style,\n",
    "                    text=customer_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8HO-EnYr5-lQ",
    "outputId": "ff554400-6b5a-4d83-d8f5-fbe955f099b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'langchain_core.messages.human.HumanMessage'>\n"
     ]
    }
   ],
   "source": [
    "print(type(customer_messages))\n",
    "print(type(customer_messages[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G-gS-xAw6BMd",
    "outputId": "c73d2b10-6b83-42ec-e4b5-c3c66ad9d2e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone\\n. text: ```\\nArrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```\\n\" additional_kwargs={} response_metadata={}\n"
     ]
    }
   ],
   "source": [
    "print(customer_messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "XYHbIPBA6FMD"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mh/dw465j0x4b9gkjqfrkgzkxlh0000gn/T/ipykernel_29534/1472147048.py:2: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  customer_response = chat(customer_messages)\n"
     ]
    }
   ],
   "source": [
    "# Call the LLM to translate to the style of the customer message\n",
    "customer_response = chat(customer_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c1bfaW1_6IX8",
    "outputId": "937ef852-42ed-4db7-c6de-4e962fc114bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm really frustrated that my blender lid flew off and splattered my kitchen walls with smoothie! And to make matters worse, the warranty doesn't cover the cost of cleaning up my kitchen. I could really use your help right now, friend.\n"
     ]
    }
   ],
   "source": [
    "print(customer_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "rkhY_ICR6Mqb"
   },
   "outputs": [],
   "source": [
    "service_reply = \"\"\"Hey there customer, \\\n",
    "the warranty does not cover \\\n",
    "cleaning expenses for your kitchen \\\n",
    "because it's your fault that \\\n",
    "you misused your blender \\\n",
    "by forgetting to put the lid on before \\\n",
    "starting the blender. \\\n",
    "Tough luck! See ya!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "EbSQgvqi6Oif"
   },
   "outputs": [],
   "source": [
    "service_style_pirate = \"\"\"\\\n",
    "a polite tone \\\n",
    "that speaks in English Pirate\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1W24xn0G6RNE",
    "outputId": "50466d17-4ce4-4012-b381-dcf6ea4cd9da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks into a style that is a polite tone that speaks in English Pirate. text: ```Hey there customer, the warranty does not cover cleaning expenses for your kitchen because it's your fault that you misused your blender by forgetting to put the lid on before starting the blender. Tough luck! See ya!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "service_messages = prompt_template.format_messages(\n",
    "    style=service_style_pirate,\n",
    "    text=service_reply)\n",
    "\n",
    "print(service_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tp5-Qod86UvB",
    "outputId": "077baa5d-23e3-49f4-b371-65a54e420bf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahoy there, valued customer! Regrettably, the warranty be not coverin' the cost o' cleanin' yer galley due to yer own negligence. Ye see, 'twas yer own doin' that ye forgot to secure the lid afore startin' the blender. 'Tis a tough break, indeed! Fare thee well!\n"
     ]
    }
   ],
   "source": [
    "service_response = chat(service_messages)\n",
    "print(service_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yRMYEub8oFN"
   },
   "source": [
    "**Output Parsers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LdxNL_2u8yd0",
    "outputId": "0ef1ba88-cde2-47b7-dfa1-fec07c81a6f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': False, 'delivery_days': 5, 'price_value': 'pretty affordable!'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"gift\": False,\n",
    "  \"delivery_days\": 5,\n",
    "  \"price_value\": \"pretty affordable!\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "2ayDS5jt82V-"
   },
   "outputs": [],
   "source": [
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing.  It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\"\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product \\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "text: {text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZSLR3PFZ8-62",
    "outputId": "a93c57aa-532d-4b90-e601-a2341e8413b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['text'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='For the following text, extract the following information:\\n\\ngift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\n\\ndelivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\nprice_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\ngift\\ndelivery_days\\nprice_value\\n\\ntext: {text}\\n'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1O5ZAsBJ9BtM",
    "outputId": "8fb50c81-0acb-4437-dbd4-5756448c2cb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"gift\": true,\n",
      "  \"delivery_days\": 2,\n",
      "  \"price_value\": \"It's slightly more expensive than the other leaf blowers out there\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "messages = prompt_template.format_messages(text=customer_review)\n",
    "chat = ChatOpenAI(openai_api_key=CURRENT_OPENAI_API_KEY, temperature=0.0)\n",
    "response = chat(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wd-fulqS9RuN",
    "outputId": "cae8f070-655d-4cb9-eaad-3dc9e0fe34a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240
    },
    "id": "jKgTqvEq9VtC",
    "outputId": "0c103cb2-7447-48ee-fb67-c49b338eb050"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"gift\": true,\\n  \"delivery_days\": 2,\\n  \"price_value\": \"It\\'s slightly more expensive than the other leaf blowers out there\"\\n}'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You will get an error by running this line of code\n",
    "# because'gift' is not a dictionary\n",
    "# 'gift' is a string\n",
    "# response.content.get('gift')\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVE3R_zJ9gpa"
   },
   "source": [
    "**Parse the LLM output string into a Python dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "HJ14Kex29j8n"
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Fc7-WwFu9nMb"
   },
   "outputs": [],
   "source": [
    "gift_schema = ResponseSchema(name=\"gift\",\n",
    "                             description=\"Was the item purchased\\\n",
    "                             as a gift for someone else? \\\n",
    "                             Answer True if yes,\\\n",
    "                             False if not or unknown.\")\n",
    "delivery_days_schema = ResponseSchema(name=\"delivery_days\",\n",
    "                                      description=\"How many days\\\n",
    "                                      did it take for the product\\\n",
    "                                      to arrive? If this \\\n",
    "                                      information is not found,\\\n",
    "                                      output -1.\")\n",
    "price_value_schema = ResponseSchema(name=\"price_value\",\n",
    "                                    description=\"Extract any\\\n",
    "                                    sentences about the value or \\\n",
    "                                    price, and output them as a \\\n",
    "                                    comma separated Python list.\")\n",
    "\n",
    "response_schemas = [gift_schema,\n",
    "                    delivery_days_schema,\n",
    "                    price_value_schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "-x4ym2Ui95vP"
   },
   "outputs": [],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "g8h2QqkI99VH"
   },
   "outputs": [],
   "source": [
    "format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VUMyQkiv-CYQ",
    "outputId": "d5c8bed0-9a52-472c-cf07-560ca2c819f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\n",
      "\t\"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\n",
      "\t\"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "-TkU51jv-LyE"
   },
   "outputs": [],
   "source": [
    "review_template_2 = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product\\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
    "\n",
    "messages = prompt.format_messages(text=customer_review,\n",
    "                                format_instructions=format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i38MXJQR-RUo",
    "outputId": "7f69e5e8-ee75-40a1-e1c2-b6f976e1e98d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the following text, extract the following information:\n",
      "\n",
      "gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
      "\n",
      "delivery_days: How many days did it take for the productto arrive? If this information is not found, output -1.\n",
      "\n",
      "price_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\n",
      "\n",
      "text: This leaf blower is pretty amazing.  It has four settings:candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\n",
      "\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\n",
      "\t\"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\n",
      "\t\"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "iypf3-TV-XW1"
   },
   "outputs": [],
   "source": [
    "response = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tL3YkFix-a8g",
    "outputId": "5774a66f-8358-4985-eb0b-e72961013f33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"gift\": true,\n",
      "\t\"delivery_days\": 2,\n",
      "\t\"price_value\": [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "mUbZ0keq-elL"
   },
   "outputs": [],
   "source": [
    "output_dict = output_parser.parse(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3U0hvm_O-hPX",
    "outputId": "6413585f-f2c8-4ee6-d6d5-56781e34de50"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': True,\n",
       " 'delivery_days': 2,\n",
       " 'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vVJteDYk-lGI",
    "outputId": "0113e50d-59df-4234-fbe5-27cbb7c28fbd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "k9kpB_Ze-m9I",
    "outputId": "9f68c963-1868-4b2f-f31e-d17a533242ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict.get('delivery_days')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Syv4duJ6eq2W"
   },
   "source": [
    "# LangChain: Memory\n",
    "\n",
    "## Outline\n",
    "- ConversationBufferMemory\n",
    "- ConversationBufferWindowMemory\n",
    "- ConversationTokenBufferMemory\n",
    "- ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajt3Pg_3gmQh"
   },
   "source": [
    "### ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "7-INhwg8eqQ-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "5lb1PwKFfIUL"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "nkhBWYjkfKvY"
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0,openai_api_key=CURRENT_OPENAI_API_KEY)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "J7IeWsCsfNnH",
    "outputId": "b15a3601-e968-4818-c0cb-188f8b22a3b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Andrew\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Andrew! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Andrew\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    },
    "id": "PrDWZwdWfPBF",
    "outputId": "adae4c3a-7170-4b29-9f90-da51b9dbf5ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Andrew\n",
      "AI: Hello Andrew! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1+1 equals 2. Is there anything else you would like to know?'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "id": "Vd27l1pVfQW6",
    "outputId": "37c715f5-676e-44a1-9424-67256329e3d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Andrew\n",
      "AI: Hello Andrew! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: 1+1 equals 2. Is there anything else you would like to know?\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Andrew. Is there anything else you would like to know, Andrew?'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "slOazeq4fRzn",
    "outputId": "0e945b0c-ecaa-4916-bfde-0b02260799c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Andrew\n",
      "AI: Hello Andrew! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: 1+1 equals 2. Is there anything else you would like to know?\n",
      "Human: What is my name?\n",
      "AI: Your name is Andrew. Is there anything else you would like to know, Andrew?\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MZrVxVnifTmU",
    "outputId": "08c37ffa-92c6-4d3f-de89-46bbc646b1f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi, my name is Andrew\\nAI: Hello Andrew! It's nice to meet you. How can I assist you today?\\nHuman: What is 1+1?\\nAI: 1+1 equals 2. Is there anything else you would like to know?\\nHuman: What is my name?\\nAI: Your name is Andrew. Is there anything else you would like to know, Andrew?\"}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "2x_noHZDiF30"
   },
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "h6UvLcBoiH01"
   },
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OVuVFEmHiJT4",
    "outputId": "9a283a33-6042-41f4-8b7b-1e0d78ba11f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zBrLOg_3iK29",
    "outputId": "5f5eae55-c5b3-4b87-9543-3aa1eb54a90a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\"}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "ST5BFP-siMYX"
   },
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YSLsDpzsiOQB",
    "outputId": "60a81d85-ae4f-4543-a84d-dc639a1f7ce0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ogP4NODzlpEy"
   },
   "source": [
    "## ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "Vw7sntZblq1a"
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "UaR2B_qtltGD"
   },
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "KWDZucA_lvH3"
   },
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pBJH5Ry5lxCK",
    "outputId": "73f505ed-61f6-4684-cc7c-4938e7b49773"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "0VTcrOmjl0Z0"
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0,openai_api_key=CURRENT_OPENAI_API_KEY)\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory = memory,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "mKEH61VFmArn",
    "outputId": "f5c8e638-794c-47c3-f584-2c6b319ed9ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Andrew! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Andrew\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "_fpgSBRUmDmU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 + 1 equals 2. Is there anything else you would like to know?'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "-KaUT4gomFkV",
    "outputId": "e1f4e40f-0a52-4f2b-c7b8-63fcea1d4ccf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I do not have access to personal information such as your name. Is there anything else you would like to know?\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsnzixk9mJ3f"
   },
   "source": [
    "## ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lr_9zPhQmLeP",
    "outputId": "bcdd8d92-dee4-4424-a525-402aefc314e5"
   },
   "outputs": [],
   "source": [
    "# !pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "O0cZ4jcImO5W"
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "llm = ChatOpenAI(temperature=0.0,openai_api_key=CURRENT_OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "eNvYmBzfmVWT"
   },
   "outputs": [],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=30)\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"},\n",
    "                    {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ei2usFmxmXTh",
    "outputId": "0efa5334-0b94-4eb1-f078-2e5895ec01a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'AI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ypuXwr9meEI"
   },
   "source": [
    "## ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "S5w-fMsmmfqj"
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "38LFee7pmhqd"
   },
   "outputs": [],
   "source": [
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"},\n",
    "                    {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p90tglo0mk0j",
    "outputId": "6af96920-96dd-496e-e180-9c99bdce5d45"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: The human and AI exchange greetings and discuss the schedule for the day, including a meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI provides details on each event and emphasizes the power of LangChain as a tool.'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "OeIpAQmnmnn9"
   },
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "id": "JhtgeaexmwcS",
    "outputId": "91181628-bd5b-49b0-cb59-1255877f6304"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human and AI exchange greetings and discuss the schedule for the day, including a meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI provides details on each event and emphasizes the power of LangChain as a tool.\n",
      "Human: What would be a good demo to show?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'For the meeting with the product team, a demo showcasing the latest features and improvements in LangChain would be ideal. This could include a live demonstration of how LangChain streamlines language translation processes, improves accuracy, and increases efficiency. Additionally, highlighting any recent success stories or case studies from clients using LangChain would be impactful. Let me know if you need more specific details or suggestions for the demo!'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a9B7rDTRm0sr",
    "outputId": "6fb7523f-e126-44ce-a23e-3eca7b0851fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: The human and AI exchange greetings and discuss the schedule for the day, including a meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI provides details on each event and emphasizes the power of LangChain as a tool. The human asks what would be a good demo to show.\\nAI: For the meeting with the product team, a demo showcasing the latest features and improvements in LangChain would be ideal. This could include a live demonstration of how LangChain streamlines language translation processes, improves accuracy, and increases efficiency. Additionally, highlighting any recent success stories or case studies from clients using LangChain would be impactful. Let me know if you need more specific details or suggestions for the demo!'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NXP9KtHQ_6bM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hjspqoph_64F"
   },
   "source": [
    "# Chains in LangChain\n",
    "\n",
    "## Outline\n",
    "\n",
    "* LLMChain\n",
    "* Sequential Chains\n",
    "  * SimpleSequentialChain\n",
    "  * SequentialChain\n",
    "* Router Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhyzLRotALUU"
   },
   "source": [
    "## LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "iLw4NPGTAN2r"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "uZzeRl4mAQcd"
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9,openai_api_key=CURRENT_OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "Qm_UHT_xAZOt"
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "t14Qd0AJAb6k"
   },
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "os1lY4mXAgGa",
    "outputId": "786b88d5-a3b5-4888-fb70-4b2bd84b8306"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Royal Dreams Bedding\"'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product = \"Queen Size Sheet Set\"\n",
    "chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NN_NvTYhAlPU"
   },
   "source": [
    "## SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "xQYWRMNHB5WI"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "MqUtE1PDB8JV"
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9,openai_api_key=CURRENT_OPENAI_API_KEY)\n",
    "\n",
    "# prompt template 1\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")\n",
    "\n",
    "# Chain 1\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "QO-BbbvBB_0E"
   },
   "outputs": [],
   "source": [
    "\n",
    "# prompt template 2\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a 20 words description for the following \\\n",
    "    company:{company_name}\"\n",
    ")\n",
    "# chain 2\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "JDiRdggWCB0F"
   },
   "outputs": [],
   "source": [
    "overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two],\n",
    "                                             verbose=True\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "id": "3d3er_6zCD6m",
    "outputId": "b96a7d94-8a60-47c4-a729-81a3a9e1c24a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\"Regal Bedding Co.\"\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mRegal Bedding Co. is a luxury bedding company that offers high-quality, stylish bedding sets and accessories for a comfortable sleep.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Regal Bedding Co. is a luxury bedding company that offers high-quality, stylish bedding sets and accessories for a comfortable sleep.'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_simple_chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCCinx2kK7l0"
   },
   "source": [
    "## SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "x7KIeH7pK67W"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "ftlhcpstK-o2"
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9,openai_api_key=CURRENT_OPENAI_API_KEY)\n",
    "\n",
    "# prompt template 1: translate to english\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate the following review to english:\"\n",
    "    \"\\n\\n{Review}\"\n",
    ")\n",
    "# chain 1: input= Review and output= English_Review\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt,\n",
    "                     output_key=\"English_Review\"\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "qeckT2-jLFJR"
   },
   "outputs": [],
   "source": [
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Can you summarize the following review in 1 sentence:\"\n",
    "    \"\\n\\n{English_Review}\"\n",
    ")\n",
    "# chain 2: input= English_Review and output= summary\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt,\n",
    "                     output_key=\"summary\"\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "jGec8yXBLGi2"
   },
   "outputs": [],
   "source": [
    "# prompt template 3: translate to english\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What language is the following review:\\n\\n{Review}\"\n",
    ")\n",
    "# chain 3: input= Review and output= language\n",
    "chain_three = LLMChain(llm=llm, prompt=third_prompt,\n",
    "                       output_key=\"language\"\n",
    "                      )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "P4PZNIQvLIr-"
   },
   "outputs": [],
   "source": [
    "\n",
    "# prompt template 4: follow up message\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a follow up response to the following \"\n",
    "    \"summary in the specified language:\"\n",
    "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
    ")\n",
    "# chain 4: input= summary, language and output= followup_message\n",
    "chain_four = LLMChain(llm=llm, prompt=fourth_prompt,\n",
    "                      output_key=\"followup_message\"\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "rN3JOW7sLOBY"
   },
   "outputs": [],
   "source": [
    "# overall_chain: input= Review\n",
    "# and output= English_Review,summary, followup_message\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "    input_variables=[\"Review\"],\n",
    "    output_variables=[\"English_Review\", \"summary\",\"followup_message\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JvDHE35pOg6-",
    "outputId": "9d71df77-03ff-4b6b-9c89-3b0cffe67541"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Review': 'Gayet iyi bir rn. ok memnunuz. Ancak paralarn kalitesi daha iyi olabilirdi. rnn ne kadar dayanaca konusunda phelerim var. ',\n",
       " 'English_Review': 'A very good product. We are very pleased. However, the quality of the parts could have been better. I have doubts about how long the product will last.',\n",
       " 'summary': 'Overall, the reviewer found the product to be good and is happy with it, but has concerns about the quality of the parts and the durability of the product in the long run.',\n",
       " 'followup_message': 'Teekkr ederim yorumunuz iin. rnn genel olarak iyi olduunu grmek sevindirici. Paralarn kalitesi ve rnn uzun vadede dayankll ile ilgili endieleriniz olduunu anlyorum. Bu konular dikkate alacaz ve daha iyiletirmeler yapmaya alacaz. Geribildiriminiz iin teekkr ederim.'}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = \"Gayet iyi bir rn. ok memnunuz. Ancak paralarn kalitesi daha iyi olabilirdi. rnn ne kadar dayanaca konusunda phelerim var. \"\n",
    "overall_chain(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uL5JVjP7PuBz"
   },
   "source": [
    "## Router Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "h2Zg0QZ6PvwG"
   },
   "outputs": [],
   "source": [
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise\\\n",
    "and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit\\\n",
    "that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. \\\n",
    "You are great at answering math questions. \\\n",
    "You are so good because you are able to break down \\\n",
    "hard problems into their component parts,\n",
    "answer the component parts, and then put them together\\\n",
    "to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "history_template = \"\"\"You are a very good historian. \\\n",
    "You have an excellent knowledge of and understanding of people,\\\n",
    "events and contexts from a range of historical periods. \\\n",
    "You have the ability to think, reflect, debate, discuss and \\\n",
    "evaluate the past. You have a respect for historical evidence\\\n",
    "and the ability to make use of it to support your explanations \\\n",
    "and judgements.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
    "You have a passion for creativity, collaboration,\\\n",
    "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
    "understanding of theories and algorithms, and excellent communication \\\n",
    "skills. You are great at answering coding questions. \\\n",
    "You are so good because you know how to solve a problem by \\\n",
    "describing the solution in imperative steps \\\n",
    "that a machine can easily interpret and you know how to \\\n",
    "choose a solution that has a good balance between \\\n",
    "time complexity and space complexity.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "5jYLB9yqPzvd"
   },
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"physics\",\n",
    "        \"description\": \"Good for answering questions about physics\",\n",
    "        \"prompt_template\": physics_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"math\",\n",
    "        \"description\": \"Good for answering math questions\",\n",
    "        \"prompt_template\": math_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"History\",\n",
    "        \"description\": \"Good for answering history questions\",\n",
    "        \"prompt_template\": history_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"computer science\",\n",
    "        \"description\": \"Good for answering computer science questions\",\n",
    "        \"prompt_template\": computerscience_template\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "XDobVmLMP_tw"
   },
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "dU3B5y3TQmrR"
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0,openai_api_key=CURRENT_OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "VDtPisq2QpR5"
   },
   "outputs": [],
   "source": [
    "\n",
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain\n",
    "\n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "xfiV5lYAQ0NY"
   },
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "PIpDOJztQ4je"
   },
   "outputs": [],
   "source": [
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
    "language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising\\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
    "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
    "well suited for any of the candidate prompts.\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "Azyt1FClQ8FI"
   },
   "outputs": [],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str\n",
    ")\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "QbAvUd6BRF2y"
   },
   "outputs": [],
   "source": [
    "chain = MultiPromptChain(router_chain=router_chain,\n",
    "                         destination_chains=destination_chains,\n",
    "                         default_chain=default_chain,\n",
    "                         verbose=True,\n",
    "                         silent_errors=True # suppress biology prompt being picked up. https://community.deeplearning.ai/t/lesson-3-chains-invalid-chain-name-biology/394466/7                        \n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "id": "Hmn1XAvARHfd",
    "outputId": "a491c553-cd5f-4883-e1f4-74550e0bd12d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "physics: {'input': 'What is black body radiation?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Black body radiation refers to the electromagnetic radiation emitted by a perfect black body, which is an idealized physical body that absorbs all incident electromagnetic radiation. The radiation emitted by a black body depends only on its temperature and follows a specific distribution known as Planck's law. This radiation is characterized by a continuous spectrum of wavelengths and intensities, with the peak intensity shifting to shorter wavelengths as the temperature of the black body increases. Black body radiation plays a key role in understanding concepts such as thermal radiation and the quantization of energy in quantum mechanics.\""
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"What is black body radiation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "XvNOFR9WRJV2",
    "outputId": "3ae3d06a-e1c2-4381-c944-a63081eff166"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "math: {'input': 'what is 2 + 2'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Thank you for the compliment! The answer to the question \"what is 2 + 2\" is 4.'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"what is 2 + 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "pQnzEwXARM-R",
    "outputId": "dab059e7-2402-47e1-ee51-ca704be0d579"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "biology: {'input': 'Why does every cell in our body contain DNA?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Every cell in our body contains DNA because DNA carries the genetic information that determines the characteristics and functions of an organism. DNA contains the instructions for building and maintaining an organism, including the proteins that are essential for cell structure and function. This genetic information is passed down from parent to offspring and is essential for the growth, development, and functioning of all cells in the body. Having DNA in every cell ensures that the genetic information is preserved and can be used to carry out the necessary processes for life.'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"Why does every cell in our body contain DNA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAfOGAs2gaGX"
   },
   "source": [
    "# LangChain: Q&A over Documents\n",
    "\n",
    "An example might be a tool that would allow you to query a product catalog for items of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "0-71vLeqggyy"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "1o6BqpjbgkJc"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from IPython.display import display, Markdown\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.3.2\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /Users/rmian/genai/langchain/venv_langchains/lib/python3.12/site-packages\n",
      "Requires: aiohttp, langchain-core, langchain-text-splitters, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: langchain-community\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "SL8QdWeb0Zfd"
   },
   "outputs": [],
   "source": [
    "file = 'OutdoorClothingCatalog_1000.csv'\n",
    "loader = CSVLoader(file_path=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "TcIjcyvt0bdg"
   },
   "outputs": [],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vC0G9p2q0dls",
    "outputId": "4d5e3cf8-8e96-4420-9e5a-ae37b930fc16"
   },
   "outputs": [],
   "source": [
    "# `pip install docarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the necessary import \n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Instantiating embeddings model \n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "qZvJgRsp0fp3"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.environ['OPENAI_API_KEY'] = CURRENT_OPENAI_API_KEY\n",
    "\n",
    "# You can use this on a terminal export OPENAI_API_KEY = CURRENT_OPENAI_API_KEY\n",
    "\n",
    "# openai.api_key = CURRENT_OPENAI_API_KEY\n",
    "index = VectorstoreIndexCreator(\n",
    "    embedding=embeddings,\n",
    "    vectorstore_cls=DocArrayInMemorySearch\n",
    "    ).from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "QpMMI6BKH0C-"
   },
   "outputs": [],
   "source": [
    "query =\"Please list all your shirts with sun protection \\\n",
    "in a table in markdown and summarize each one.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "ChuSGHFiH15u"
   },
   "outputs": [],
   "source": [
    "llm_model = OpenAI() # \"gpt-3.5-turbo\"\n",
    "response = index.query(query, llm=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mOpenAI\u001b[0m\n",
      "Params: {'model_name': 'gpt-3.5-turbo-instruct', 'temperature': 0.7, 'top_p': 1.0, 'frequency_penalty': 0.0, 'presence_penalty': 0.0, 'n': 1, 'logit_bias': {}, 'max_tokens': 256}\n"
     ]
    }
   ],
   "source": [
    "print(llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "id": "eS71SZOMH3c1",
    "outputId": "e343a049-a232-41cf-fdf3-954a46702465"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "| Shirt Name | Sun Protection | Description |\n",
       "| --- | --- | --- |\n",
       "| Men's Tropical Plaid Short-Sleeve Shirt | UPF 50+ | Made of 100% polyester, wrinkle-resistant, front and back cape venting, two front bellows pockets, imported, provides highest rated sun protection possible |\n",
       "| Men's Plaid Tropic Shirt, Short-Sleeve | UPF 50+ | Made of 52% polyester and 48% nylon, machine washable and dryable, front and back cape venting, two front bellows pockets, imported, offers extended travel sun protection |\n",
       "| Men's TropicVibe Shirt, Short-Sleeve | UPF 50+ | Made of 71% nylon and 29% polyester, lined with 100% polyester knit mesh, machine washable and dryable, front and back cape venting, two front bellows pockets, imported, has lightweight feel and guaranteed sun protection |\n",
       "| Sun Shield Shirt | UPF 50+ | Made of 78% nylon and 22% Lycra Xtra Life fiber, handwash and line dry, wicks moisture for quick-drying comfort, fits comfortably over swimsuit, abrasion resistant, imported, recommended"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "MD7CG0v5H5bK"
   },
   "outputs": [],
   "source": [
    "loader = CSVLoader(file_path=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "-2YkNvQ2H9i8"
   },
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'OutdoorClothingCatalog_1000.csv', 'row': 0}, page_content=\": 0\\nname: Women's Campside Oxfords\\ndescription: This ultracomfortable lace-to-toe Oxford boasts a super-soft canvas, thick cushioning, and quality construction for a broken-in feel from the first time you put them on. \\n\\nSize & Fit: Order regular shoe size. For half sizes not offered, order up to next whole size. \\n\\nSpecs: Approx. weight: 1 lb.1 oz. per pair. \\n\\nConstruction: Soft canvas material for a broken-in feel and look. Comfortable EVA innersole with Cleansport NXT antimicrobial odor control. Vintage hunt, fish and camping motif on innersole. Moderate arch contour of innersole. EVA foam midsole for cushioning and support. Chain-tread-inspired molded rubber outsole with modified chain-tread pattern. Imported. \\n\\nQuestions? Please contact us for any inquiries.\")"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "kjddY4lnICea"
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "GjP_eZqGIEJu"
   },
   "outputs": [],
   "source": [
    "embed = embeddings.embed_query(\"Hi my name is Harrison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6CFhr7YuIGi-",
    "outputId": "a4a9c416-5318-4837-92ec-edf1b00bdf45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n"
     ]
    }
   ],
   "source": [
    "print(len(embed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "azQkdZVXIIMT",
    "outputId": "729517cc-6abc-46dd-c0b9-c7d31745af52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.021990482046978704, 0.0067465087335481025, -0.018174780766530486, -0.03918623602138189, -0.014045289898302843]\n"
     ]
    }
   ],
   "source": [
    "print(embed[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "GntqpSYKIKey"
   },
   "outputs": [],
   "source": [
    "db = DocArrayInMemorySearch.from_documents(\n",
    "    docs,\n",
    "    embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "BIQMFBq0IMR6"
   },
   "outputs": [],
   "source": [
    "query = \"Please suggest a shirt with sunblocking\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "V86dKmO6IONm"
   },
   "outputs": [],
   "source": [
    "docs = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qO-gyaMbIQ9a",
    "outputId": "17fabecc-3e7b-49e2-d16b-aed62b97514a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qe8TYZaGITNh",
    "outputId": "6ee208d3-0caf-4919-d269-0894343ae193"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'OutdoorClothingCatalog_1000.csv', 'row': 255}, page_content=': 255\\nname: Sun Shield Shirt by\\ndescription: \"Block the sun, not the fun  our high-performance sun shirt is guaranteed to protect from harmful UV rays. \\n\\nSize & Fit: Slightly Fitted: Softly shapes the body. Falls at hip.\\n\\nFabric & Care: 78% nylon, 22% Lycra Xtra Life fiber. UPF 50+ rated  the highest rated sun protection possible. Handwash, line dry.\\n\\nAdditional Features: Wicks moisture for quick-drying comfort. Fits comfortably over your favorite swimsuit. Abrasion resistant for season after season of wear. Imported.\\n\\nSun Protection That Won\\'t Wear Off\\nOur high-performance fabric provides SPF 50+ sun protection, blocking 98% of the sun\\'s harmful rays. This fabric is recommended by The Skin Cancer Foundation as an effective UV protectant.')"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "ceuRtniaIVV_"
   },
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "C7Bll3BYIbiG"
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "vanNHwDqIeFW"
   },
   "outputs": [],
   "source": [
    "qdocs = \"\".join([docs[i].page_content for i in range(len(docs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "kqPBHFl4IiWA"
   },
   "outputs": [],
   "source": [
    "response = llm.call_as_llm(f\"{qdocs} Question: Please list all your \\\n",
    "shirts with sun protection in a table in markdown and summarize each one.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "Nn_xQZO9Il5Y",
    "outputId": "cc55d9d7-58f3-4203-a85b-69a1df8e715c"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Name                           | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "phmMZOmtIntX"
   },
   "outputs": [],
   "source": [
    "qa_stuff = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "hRwWACS9IqDa"
   },
   "outputs": [],
   "source": [
    "query =  \"Please list all your shirts with sun protection in a table \\\n",
    "in markdown and summarize each one.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_P554J69Ir2x",
    "outputId": "d55feffa-ef8d-4057-e080-ec69871b32e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = qa_stuff.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "1vwEUEx0IuMw",
    "outputId": "5e2481e0-95dd-4c30-e99d-c2ed69d238ea"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Shirt Name                           | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "Ho2d1u0wIwgi"
   },
   "outputs": [],
   "source": [
    "response = index.query(query, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "EDHRLNoiIzAt"
   },
   "outputs": [],
   "source": [
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch,\n",
    "    embedding=embeddings,\n",
    ").from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "| Name | Description | Sun Protection Rating |\n",
       "| --- | --- | --- |\n",
       "| Men's Tropical Plaid Short-Sleeve Shirt | Made of 100% polyester, UPF 50+ rating, front and back cape venting, two front bellows pockets | SPF 50+, blocks 98% of harmful UV rays |\n",
       "| Men's Plaid Tropic Shirt, Short-Sleeve | Made of 52% polyester and 48% nylon, UPF 50+ rating, front and back cape venting, two front bellows pockets | SPF 50+, blocks 98% of harmful UV rays |\n",
       "| Men's TropicVibe Shirt, Short-Sleeve | Made of 71% nylon and 29% polyester, UPF 50+ rating, front and back cape venting, two front bellows pockets | SPF 50+, blocks 98% of harmful UV rays |\n",
       "| Sun Shield Shirt | Made of 78% nylon and 22% Lycra Xtra Life fiber, UPF 50+ rating, wicks moisture, abrasion resistant | SPF 50+, blocks 98% of harmful UV rays |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm_replacement_model = OpenAI(temperature=0, \n",
    "                               model='gpt-3.5-turbo-instruct')\n",
    "response = index.query(query, \n",
    "                       llm = llm_replacement_model)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDjf6e_bK913"
   },
   "source": [
    "# LangChain: Evaluation\n",
    "\n",
    "## Outline:\n",
    "\n",
    "* Example generation\n",
    "* Manual evaluation (and debuging)\n",
    "* LLM-assisted evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "njhubZyuLBs_"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBi8doAiLD0q"
   },
   "source": [
    "## Create QandA application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "id": "jR2BcBmvLFvs"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import DocArrayInMemorySearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "YGXtVvLGLPn2"
   },
   "outputs": [],
   "source": [
    "file = 'OutdoorClothingCatalog_1000.csv'\n",
    "loader = CSVLoader(file_path=file)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "l2-sswsoLW19"
   },
   "outputs": [],
   "source": [
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch,\n",
    "    embedding=embeddings\n",
    ").from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "S6tZdppnYKZh"
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature = 0.0)\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=index.vectorstore.as_retriever(),\n",
    "    verbose=True,\n",
    "    chain_type_kwargs = {\n",
    "        \"document_separator\": \"<<<<>>>>>\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8_8cT5kYORq"
   },
   "source": [
    "### Coming up with test datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GI9bz_ZFYQgQ",
    "outputId": "03c74c2b-1520-4d7a-acf2-9dfa7a35b49e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'OutdoorClothingCatalog_1000.csv', 'row': 10}, page_content=\": 10\\nname: Cozy Comfort Pullover Set, Stripe\\ndescription: Perfect for lounging, this striped knit set lives up to its name. We used ultrasoft fabric and an easy design that's as comfortable at bedtime as it is when we have to make a quick run out.\\n\\nSize & Fit\\n- Pants are Favorite Fit: Sits lower on the waist.\\n- Relaxed Fit: Our most generous fit sits farthest from the body.\\n\\nFabric & Care\\n- In the softest blend of 63% polyester, 35% rayon and 2% spandex.\\n\\nAdditional Features\\n- Relaxed fit top with raglan sleeves and rounded hem.\\n- Pull-on pants have a wide elastic waistband and drawstring, side pockets and a modern slim leg.\\n\\nImported.\")"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WqZgBHJaYTqv",
    "outputId": "7dafe23d-b40c-40d4-ed5c-37d48e49f4ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'OutdoorClothingCatalog_1000.csv', 'row': 11}, page_content=': 11\\nname: Ultra-Lofty 850 Stretch Down Hooded Jacket\\ndescription: This technical stretch down jacket from our DownTek collection is sure to keep you warm and comfortable with its full-stretch construction providing exceptional range of motion. With a slightly fitted style that falls at the hip and best with a midweight layer, this jacket is suitable for light activity up to 20 and moderate activity up to -30. The soft and durable 100% polyester shell offers complete windproof protection and is insulated with warm, lofty goose down. Other features include welded baffles for a no-stitch construction and excellent stretch, an adjustable hood, an interior media port and mesh stash pocket and a hem drawcord. Machine wash and dry. Imported.')"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xoLUXqQSYWvV"
   },
   "source": [
    "### Hard-coded examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "id": "BNCO6un-YYrK"
   },
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"query\": \"Do the Cozy Comfort Pullover Set\\\n",
    "        have side pockets?\",\n",
    "        \"answer\": \"Yes\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What collection is the Ultra-Lofty \\\n",
    "        850 Stretch Down Hooded Jacket from?\",\n",
    "        \"answer\": \"The DownTek collection\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eu1C3Cj7Ycna"
   },
   "source": [
    "### LLM-Generated examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "9_flneKIYeSL"
   },
   "outputs": [],
   "source": [
    "from langchain.evaluation.qa import QAGenerateChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "2gHpkTHOYgVj"
   },
   "outputs": [],
   "source": [
    "example_gen_chain = QAGenerateChain.from_llm(ChatOpenAI())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eimcPlaeYiS9",
    "outputId": "82382638-b80b-459f-bbeb-0c62833c8099"
   },
   "outputs": [],
   "source": [
    "new_examples = example_gen_chain.apply_and_parse(\n",
    "    [{\"doc\": t} for t in data[:5]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p6lhzq8OYkbb",
    "outputId": "f5d270a0-8b64-4cb7-afe3-bd4a56a44433"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qa_pairs': {'query': \"What is the approximate weight of the Women's Campside Oxfords per pair?\",\n",
       "  'answer': \"The approximate weight of the Women's Campside Oxfords per pair is 1 lb. 1 oz.\"}}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rbb8p5hyYl93",
    "outputId": "f01de829-7f81-4af4-ce7f-72749305c9f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'OutdoorClothingCatalog_1000.csv', 'row': 0}, page_content=\": 0\\nname: Women's Campside Oxfords\\ndescription: This ultracomfortable lace-to-toe Oxford boasts a super-soft canvas, thick cushioning, and quality construction for a broken-in feel from the first time you put them on. \\n\\nSize & Fit: Order regular shoe size. For half sizes not offered, order up to next whole size. \\n\\nSpecs: Approx. weight: 1 lb.1 oz. per pair. \\n\\nConstruction: Soft canvas material for a broken-in feel and look. Comfortable EVA innersole with Cleansport NXT antimicrobial odor control. Vintage hunt, fish and camping motif on innersole. Moderate arch contour of innersole. EVA foam midsole for cushioning and support. Chain-tread-inspired molded rubber outsole with modified chain-tread pattern. Imported. \\n\\nQuestions? Please contact us for any inquiries.\")"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PdaXRkM7Yo4X"
   },
   "source": [
    "### Combine examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "29wPIMIUYq1L"
   },
   "outputs": [],
   "source": [
    "examples += new_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "gPBHuPw-YtLl",
    "outputId": "2c063b71-5c28-4f8e-a8e7-ac64ee64739a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Yes, the Cozy Comfort Pullover Set does have side pockets.'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(examples[0][\"query\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7SSNBXcYwp_"
   },
   "source": [
    "## Manual Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "HAP0vsbnYyED"
   },
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MmZ6TAqfY0JS",
    "outputId": "62a14b14-a0eb-4f24-e6c5-ec029bfccc4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RetrievalQA] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"query\": \"Do the Cozy Comfort Pullover Set        have side pockets?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RetrievalQA > chain:StuffDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RetrievalQA > chain:StuffDocumentsChain > chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Do the Cozy Comfort Pullover Set        have side pockets?\",\n",
      "  \"context\": \": 10\\nname: Cozy Comfort Pullover Set, Stripe\\ndescription: Perfect for lounging, this striped knit set lives up to its name. We used ultrasoft fabric and an easy design that's as comfortable at bedtime as it is when we have to make a quick run out.\\n\\nSize & Fit\\n- Pants are Favorite Fit: Sits lower on the waist.\\n- Relaxed Fit: Our most generous fit sits farthest from the body.\\n\\nFabric & Care\\n- In the softest blend of 63% polyester, 35% rayon and 2% spandex.\\n\\nAdditional Features\\n- Relaxed fit top with raglan sleeves and rounded hem.\\n- Pull-on pants have a wide elastic waistband and drawstring, side pockets and a modern slim leg.\\n\\nImported.<<<<>>>>>: 73\\nname: Cozy Cuddles Knit Pullover Set\\ndescription: Perfect for lounging, this knit set lives up to its name. We used ultrasoft fabric and an easy design that's as comfortable at bedtime as it is when we have to make a quick run out. \\n\\nSize & Fit \\nPants are Favorite Fit: Sits lower on the waist. \\nRelaxed Fit: Our most generous fit sits farthest from the body. \\n\\nFabric & Care \\nIn the softest blend of 63% polyester, 35% rayon and 2% spandex.\\n\\nAdditional Features \\nRelaxed fit top with raglan sleeves and rounded hem. \\nPull-on pants have a wide elastic waistband and drawstring, side pockets and a modern slim leg. \\nImported.<<<<>>>>>: 632\\nname: Cozy Comfort Fleece Pullover\\ndescription: The ultimate sweater fleece  made from superior fabric and offered at an unbeatable price. \\n\\nSize & Fit\\nSlightly Fitted: Softly shapes the body. Falls at hip. \\n\\nWhy We Love It\\nOur customers (and employees) love the rugged construction and heritage-inspired styling of our popular Sweater Fleece Pullover and wear it for absolutely everything. From high-intensity activities to everyday tasks, you'll find yourself reaching for it every time.\\n\\nFabric & Care\\nRugged sweater-knit exterior and soft brushed interior for exceptional warmth and comfort. Made from soft, 100% polyester. Machine wash and dry.\\n\\nAdditional Features\\nFeatures our classic Mount Katahdin logo. Snap placket. Front princess seams create a feminine shape. Kangaroo handwarmer pockets. Cuffs and hem reinforced with jersey binding. Imported.\\n\\n  Official Supplier to the U.S. Ski Team\\nTHEIR WILL TO WIN, WOVEN RIGHT IN. LEARN MORE<<<<>>>>>: 151\\nname: Cozy Quilted Sweatshirt\\ndescription: Our sweatshirt is an instant classic with its great quilted texture and versatile weight that easily transitions between seasons. With a traditional fit that is relaxed through the chest, sleeve, and waist, this pullover is lightweight enough to be worn most months of the year. The cotton blend fabric is super soft and comfortable, making it the perfect casual layer. To make dressing easy, this sweatshirt also features a snap placket and a heritage-inspired Mt. Katahdin logo patch. For care, machine wash and dry. Imported.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RetrievalQA > chain:StuffDocumentsChain > chain:LLMChain > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n: 10\\nname: Cozy Comfort Pullover Set, Stripe\\ndescription: Perfect for lounging, this striped knit set lives up to its name. We used ultrasoft fabric and an easy design that's as comfortable at bedtime as it is when we have to make a quick run out.\\n\\nSize & Fit\\n- Pants are Favorite Fit: Sits lower on the waist.\\n- Relaxed Fit: Our most generous fit sits farthest from the body.\\n\\nFabric & Care\\n- In the softest blend of 63% polyester, 35% rayon and 2% spandex.\\n\\nAdditional Features\\n- Relaxed fit top with raglan sleeves and rounded hem.\\n- Pull-on pants have a wide elastic waistband and drawstring, side pockets and a modern slim leg.\\n\\nImported.<<<<>>>>>: 73\\nname: Cozy Cuddles Knit Pullover Set\\ndescription: Perfect for lounging, this knit set lives up to its name. We used ultrasoft fabric and an easy design that's as comfortable at bedtime as it is when we have to make a quick run out. \\n\\nSize & Fit \\nPants are Favorite Fit: Sits lower on the waist. \\nRelaxed Fit: Our most generous fit sits farthest from the body. \\n\\nFabric & Care \\nIn the softest blend of 63% polyester, 35% rayon and 2% spandex.\\n\\nAdditional Features \\nRelaxed fit top with raglan sleeves and rounded hem. \\nPull-on pants have a wide elastic waistband and drawstring, side pockets and a modern slim leg. \\nImported.<<<<>>>>>: 632\\nname: Cozy Comfort Fleece Pullover\\ndescription: The ultimate sweater fleece  made from superior fabric and offered at an unbeatable price. \\n\\nSize & Fit\\nSlightly Fitted: Softly shapes the body. Falls at hip. \\n\\nWhy We Love It\\nOur customers (and employees) love the rugged construction and heritage-inspired styling of our popular Sweater Fleece Pullover and wear it for absolutely everything. From high-intensity activities to everyday tasks, you'll find yourself reaching for it every time.\\n\\nFabric & Care\\nRugged sweater-knit exterior and soft brushed interior for exceptional warmth and comfort. Made from soft, 100% polyester. Machine wash and dry.\\n\\nAdditional Features\\nFeatures our classic Mount Katahdin logo. Snap placket. Front princess seams create a feminine shape. Kangaroo handwarmer pockets. Cuffs and hem reinforced with jersey binding. Imported.\\n\\n  Official Supplier to the U.S. Ski Team\\nTHEIR WILL TO WIN, WOVEN RIGHT IN. LEARN MORE<<<<>>>>>: 151\\nname: Cozy Quilted Sweatshirt\\ndescription: Our sweatshirt is an instant classic with its great quilted texture and versatile weight that easily transitions between seasons. With a traditional fit that is relaxed through the chest, sleeve, and waist, this pullover is lightweight enough to be worn most months of the year. The cotton blend fabric is super soft and comfortable, making it the perfect casual layer. To make dressing easy, this sweatshirt also features a snap placket and a heritage-inspired Mt. Katahdin logo patch. For care, machine wash and dry. Imported.\\nHuman: Do the Cozy Comfort Pullover Set        have side pockets?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RetrievalQA > chain:StuffDocumentsChain > chain:LLMChain > llm:ChatOpenAI] [651ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Yes, the Cozy Comfort Pullover Set does have side pockets.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Yes, the Cozy Comfort Pullover Set does have side pockets.\",\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 14,\n",
      "                \"prompt_tokens\": 733,\n",
      "                \"total_tokens\": 747,\n",
      "                \"completion_tokens_details\": {\n",
      "                  \"audio_tokens\": null,\n",
      "                  \"reasoning_tokens\": 0\n",
      "                },\n",
      "                \"prompt_tokens_details\": {\n",
      "                  \"audio_tokens\": null,\n",
      "                  \"cached_tokens\": 0\n",
      "                }\n",
      "              },\n",
      "              \"model_name\": \"gpt-3.5-turbo\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": null\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-fc73f597-e1bb-4108-afa4-6303399227f3-0\",\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 14,\n",
      "      \"prompt_tokens\": 733,\n",
      "      \"total_tokens\": 747,\n",
      "      \"completion_tokens_details\": {\n",
      "        \"audio_tokens\": null,\n",
      "        \"reasoning_tokens\": 0\n",
      "      },\n",
      "      \"prompt_tokens_details\": {\n",
      "        \"audio_tokens\": null,\n",
      "        \"cached_tokens\": 0\n",
      "      }\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RetrievalQA > chain:StuffDocumentsChain > chain:LLMChain] [654ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"Yes, the Cozy Comfort Pullover Set does have side pockets.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RetrievalQA > chain:StuffDocumentsChain] [655ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \"Yes, the Cozy Comfort Pullover Set does have side pockets.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RetrievalQA] [919ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"result\": \"Yes, the Cozy Comfort Pullover Set does have side pockets.\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Yes, the Cozy Comfort Pullover Set does have side pockets.'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(examples[0][\"query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "id": "bmsKQiXMY4OW"
   },
   "outputs": [],
   "source": [
    "# Turn off the debug mode\n",
    "langchain.debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPs5Q_KmY65t"
   },
   "source": [
    "## LLM assisted evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = qa.apply(examples) # broken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "modified_examples = [example['qa_pairs'] for example in examples[2:]] #https://community.deeplearning.ai/t/lesson-5-update-request-qageneratechain/445255\n",
    "# we add the hardcoded examples again to modified_examples\n",
    "predictions = qa.apply(modified_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "id": "mkUV1a5vZAmB"
   },
   "outputs": [],
   "source": [
    "from langchain.evaluation.qa import QAEvalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "id": "y-PoPbBbZCb3"
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0)\n",
    "eval_chain = QAEvalChain.from_llm(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "id": "7mccaURvZEeX"
   },
   "outputs": [],
   "source": [
    "graded_outputs = eval_chain.evaluate(modified_examples, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'results': 'CORRECT'}"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graded_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KRXK-4pZZGcq",
    "outputId": "ec6cf6ba-1c5c-47ea-e2e3-2960df94ce3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0:\n",
      "Question: What is the approximate weight of the Women's Campside Oxfords per pair?\n",
      "Real Answer: The approximate weight of the Women's Campside Oxfords per pair is 1 lb. 1 oz.\n",
      "Predicted Answer: The approximate weight of the Women's Campside Oxfords per pair is 1 lb. 1 oz.\n",
      "Predicted Grade: CORRECT\n",
      "\n",
      "Example 1:\n",
      "Question: What are the dimensions of the small and medium sizes for the Recycled Waterhog Dog Mat in the Chevron Weave design?\n",
      "Real Answer: The small size of the mat has dimensions of 18\" x 28\", while the medium size has dimensions of 22.5\" x 34.5\".\n",
      "Predicted Answer: The dimensions for the small size of the Recycled Waterhog Dog Mat in the Chevron Weave design are 18\" x 28\", and for the medium size, they are 22.5\" x 34.5\".\n",
      "Predicted Grade: CORRECT\n",
      "\n",
      "Example 2:\n",
      "Question: What are some key features of the Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece as described in the document?\n",
      "Real Answer: Some key features of the swimsuit include bright colors, ruffles, exclusive whimsical prints, four-way-stretch and chlorine-resistant fabric, UPF 50+ sun protection rating, crossover no-slip straps, fully lined bottom for secure fit and maximum coverage, and the recommendation to machine wash and line dry for best results.\n",
      "Predicted Answer: Some key features of the Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece are:\n",
      "- Bright colors, ruffles, and exclusive whimsical prints\n",
      "- Four-way-stretch and chlorine-resistant fabric\n",
      "- UPF 50+ rated fabric for high sun protection\n",
      "- Crossover no-slip straps and fully lined bottom for a secure fit and maximum coverage\n",
      "- Machine washable and line dry for best results\n",
      "Predicted Grade: CORRECT\n",
      "\n",
      "Example 3:\n",
      "Question: What is the fabric composition of the Refresh Swimwear V-Neck Tankini Contrasts top?\n",
      "Real Answer: The body of the swimtop is made of 82% recycled nylon and 18% Lycra spandex, while the lining is made of 90% recycled nylon and 10% Lycra spandex.\n",
      "Predicted Answer: The fabric composition of the Refresh Swimwear V-Neck Tankini Contrasts top is 82% recycled nylon with 18% Lycra spandex for the body, and the lining is 90% recycled nylon with 10% Lycra spandex.\n",
      "Predicted Grade: CORRECT\n",
      "\n",
      "Example 4:\n",
      "Question: What technology makes the EcoFlex 3L Storm Pants more breathable and waterproof?\n",
      "Real Answer: The EcoFlex 3L Storm Pants feature TEK O2 technology, which offers the most breathability tested and provides enhanced waterproof protection.\n",
      "Predicted Answer: The EcoFlex 3L Storm Pants use TEK O2 technology to make them more breathable and waterproof.\n",
      "Predicted Grade: CORRECT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, eg in enumerate(modified_examples):\n",
    "    print(f\"Example {i}:\")\n",
    "    print(\"Question: \" + predictions[i]['query'])\n",
    "    print(\"Real Answer: \" + predictions[i]['answer'])\n",
    "    print(\"Predicted Answer: \" + predictions[i]['result'])\n",
    "    print(\"Predicted Grade: \" + graded_outputs[i]['results'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfDY8yaUWHqm"
   },
   "source": [
    "# LangChain: Agents\n",
    "\n",
    "## Outline:\n",
    "\n",
    "* Using built in LangChain tools: DuckDuckGo search and Wikipedia\n",
    "* Defining your own tools\n",
    "\n",
    "\n",
    "Note that agents are work in progress as of July 2023. Check the documentation at LangChain.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "id": "pxGCk_5gWL2J"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6BrP_0dWQKd"
   },
   "source": [
    "## Built-in LangChain tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZeMxgnUMWSH1",
    "outputId": "02ac616f-2a48-4d99-863b-8b1d2e3bc6f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting duckduckgo-search\n",
      "  Downloading duckduckgo_search-6.3.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: wikipedia in /Users/rmian/genai/langchain/venv_langchains/lib/python3.12/site-packages (1.4.0)\n",
      "Collecting click>=8.1.7 (from duckduckgo-search)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting primp>=0.6.3 (from duckduckgo-search)\n",
      "  Downloading primp-0.6.3-cp38-abi3-macosx_10_12_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/rmian/genai/langchain/venv_langchains/lib/python3.12/site-packages (from wikipedia) (4.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /Users/rmian/genai/langchain/venv_langchains/lib/python3.12/site-packages (from wikipedia) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/rmian/genai/langchain/venv_langchains/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/rmian/genai/langchain/venv_langchains/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/rmian/genai/langchain/venv_langchains/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/rmian/genai/langchain/venv_langchains/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.8.30)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/rmian/genai/langchain/venv_langchains/lib/python3.12/site-packages (from beautifulsoup4->wikipedia) (2.6)\n",
      "Downloading duckduckgo_search-6.3.0-py3-none-any.whl (27 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading primp-0.6.3-cp38-abi3-macosx_10_12_x86_64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: primp, click, duckduckgo-search\n",
      "Successfully installed click-8.1.7 duckduckgo-search-6.3.0 primp-0.6.3\n"
     ]
    }
   ],
   "source": [
    "!pip install -U duckduckgo-search wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "id": "pxXl4q3rWUhO"
   },
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools, initialize_agent\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import AgentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "id": "dtQHn9T-WWC4"
   },
   "outputs": [],
   "source": [
    "\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key=CURRENT_OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "id": "oRzywEwtWXol"
   },
   "outputs": [],
   "source": [
    "tools = load_tools([\"ddg-search\", \"wikipedia\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "id": "eDFUgi1EWZBg"
   },
   "outputs": [],
   "source": [
    "agent= initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    handle_parsing_errors=True,\n",
    "    verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69CKiiBqWbuu"
   },
   "source": [
    "**Note**:\n",
    "\n",
    "Accessing the external DuckDuckGo agent can sometimes cause an exception.\n",
    "\n",
    "If you encounter this, please try it again, possibly when there is reduced load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k8hwg5vsWdNH",
    "outputId": "7e818780-5478-4e38-f965-783dff052af9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to search for the winner of the 2022 World Cup.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"duckduckgo_search\",\n",
      "  \"action_input\": \"2022 World Cup winner\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m Argentina v France: 2022 FIFA World Cup Final (December 18, 2022)Relive the 2022 #FIFAWorldCup on FIFA+  https://www.plus.fifa.com/en/showcase/2725ef5... Argentina 3-3 France (4-2 PSO) Qatar 2022 | Final Lusail Stadium Attendance: 88,966 Goals: Messi (23 pen and 108), Di Maria (36); Mbappe (80 pen, 81 and 118 pen) Watch the highlights from the ... Open Extended Reactions. On Dec. 18, 2022, Argentina won the men's World Cup in the most dramatic way possible, beating France in a penalty shootout after a breathless 3-3 draw in Lusail, Qatar ... 2022 FIFA World Cup, international football (soccer) tournament that took place in Qatar from November 20 to December 18, 2022, and was contested by the men's national teams of 32 countries. Argentina won its third World Cup victory in the tournament after defeating France in the final match. The. While searching our database we found 1 possible solution for the: 2022 World Cup winner: Abbr. crossword clue. This crossword clue was last seen on May 19 2024 LA Times Crossword puzzle.The solution we have for 2022 World Cup winner: Abbr. has a total of 3 letters.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: Argentina\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    agent(\"who won the 2022 world cup\")\n",
    "except:\n",
    "    print(\"exception on external access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aaCTLl_YWe05",
    "outputId": "5261e12a-0769-4a2f-be19-4c4a405bf1ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mCould not parse LLM output: Thought: I should search for Tom M. Mitchell's notable works to find out which book he wrote.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": wikipedia,\n",
      "  \"action_input\": \"Tom M. Mitchell\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:\u001b[32;1m\u001b[1;3mI will try searching for Tom M. Mitchell's notable works using DuckDuckGo Search instead of Wikipedia.\n",
      "Action: \n",
      "```\n",
      "{\n",
      "  \"action\": \"duckduckgo_search\",\n",
      "  \"action_input\": \"Tom M. Mitchell notable works\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mMitchell is a celebrated leader in machine learning, through both his research and his organizational efforts. Important machine learning research contributions include: Semi-supervised learning from unlabelled data by co-training, with applications to unstructured text and hypertext. Development of machine learning algorithms for analysis of brain image data (fMRI and MEG), producing models ... View the Carnegie Mellon University profile of Tom Mitchell. Including their publications and teaching activities. All experts. ABOUT. SCHOLARLY & CREATIVE WORKS ... SCS Founders University Professor. Machine Learning (412) 268-2611 (Work) mitchell@andrew.cmu.edu. mitchell@cs.cmu.edu (Work) Carnegie Mellon University, Machine Learning ... Tom M. Mitchell is the Founders University Professor at Carnegie Mellon University, where he founded the world's first Machine Learning Department. His research interests include machine learning, artificial intelligence, cognitive neuroscience, and the impact of AI on society. A current focus is on AI to improve online education, where his ... One of the paradigmatic works leveraging decoding analysis in language and speech is ... However, when the attention vectors from all heads were concatenated and examined collectively, there was a notable surge in the F1 score, exceeding 0.9. ... Mitchell et al. (2008) Tom M Mitchell, Svetlana V Shinkareva, Andrew Carlson, Kai-Min Chang ... Graduate Reading and Research. 16 May 2022 - 15 Aug 2022. 10920. 1. 2. 7. View the Carnegie Mellon University profile of Tom Mitchell. Including their publications and teaching activities.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI will search for Tom M. Mitchell's notable works mentioned in the search results to find out which book he wrote.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"duckduckgo_search\",\n",
      "  \"action_input\": \"Tom M. Mitchell notable works book\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mView the Carnegie Mellon University profile of Tom Mitchell. Including their publications and teaching activities. All experts. ABOUT. SCHOLARLY & CREATIVE WORKS ... SCS Founders University Professor. Machine Learning (412) 268-2611 (Work) mitchell@andrew.cmu.edu. mitchell@cs.cmu.edu (Work) Carnegie Mellon University, Machine Learning ... Mitchell is a celebrated leader in machine learning, through both his research and his organizational efforts. Important machine learning research contributions include: Semi-supervised learning from unlabelled data by co-training, with applications to unstructured text and hypertext. Development of machine learning algorithms for analysis of brain image data (fMRI and MEG), producing models ... Tom M. Mitchell's \"Machine Learning\" is widely recognized for its accessible writing style. Even if you're new to the subject, the book doesn't overwhelm you with technical jargon or dense ... Interview by Sophie Roell, Editor. The AI Playbook: Mastering the Rare Art of Machine Learning Deployment. by Eric Siegel. 1 Evil Robots, Killer Computers, and Other Myths: The Truth About AI and the Future of Humanity by Steve Shwartz. 2 Mining Your Own Business: A Primer for Executives on Understanding and Employing Data Mining and Predictive ... Machine Learning Books of All Time. Our goal: Find the best Machine Learning books according to the internet (not just one random person's opinion).. Here's what we did:; Type \"best machine learning books\" into our search engine and study the top 5+ pages.; Add only the books mentioned 2+ times.; Rank the results neatly for you here!  (It was a lot of work. But hey!\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mCould not parse LLM output: Final Answer\u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:\u001b[32;1m\u001b[1;3mI will search for Tom M. Mitchell's notable works using DuckDuckGo Search to specifically find out the book he wrote.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"duckduckgo_search\",\n",
      "  \"action_input\": \"Tom M. Mitchell notable works book\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mView the Carnegie Mellon University profile of Tom Mitchell. Including their publications and teaching activities. All experts. ABOUT. SCHOLARLY & CREATIVE WORKS ... SCS Founders University Professor. Machine Learning (412) 268-2611 (Work) mitchell@andrew.cmu.edu. mitchell@cs.cmu.edu (Work) Carnegie Mellon University, Machine Learning ... Mitchell is a celebrated leader in machine learning, through both his research and his organizational efforts. Important machine learning research contributions include: Semi-supervised learning from unlabelled data by co-training, with applications to unstructured text and hypertext. Development of machine learning algorithms for analysis of brain image data (fMRI and MEG), producing models ... Tom M Mitchell, The Need for Biases in Learning Generalizations (Department of Computer Science, Laboratory for Computer Science Research , 1980). If we could perfectly mathematically describe the data generating process behind the physical characteristics of irises (or any other population), we wouldn't need a dataset  we could ... Interview by Sophie Roell, Editor. The AI Playbook: Mastering the Rare Art of Machine Learning Deployment. by Eric Siegel. 1 Evil Robots, Killer Computers, and Other Myths: The Truth About AI and the Future of Humanity by Steve Shwartz. 2 Mining Your Own Business: A Primer for Executives on Understanding and Employing Data Mining and Predictive ... Tom M. Mitchell's \"Machine Learning\" is widely recognized for its accessible writing style. Even if you're new to the subject, the book doesn't overwhelm you with technical jargon or dense ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: Tom M. Mitchell wrote the book \"Machine Learning.\"\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "question = \"Tom M. Mitchell is an American computer scientist \\\n",
    "and the Founders University Professor at Carnegie Mellon University (CMU)\\\n",
    "what book did he write?\"\n",
    "\n",
    "try:\n",
    "    result = agent(question)\n",
    "except:\n",
    "    print(\"exception on external access\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxHV_f65WiZI"
   },
   "source": [
    "## Define your own tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pyP47s7pWj2a",
    "outputId": "3d24643a-e626-4ff3-af01-6cb371a8565b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting DateTime\n",
      "  Using cached DateTime-5.5-py3-none-any.whl.metadata (33 kB)\n",
      "Collecting zope.interface (from DateTime)\n",
      "  Downloading zope.interface-7.0.3-cp312-cp312-macosx_10_9_x86_64.whl.metadata (43 kB)\n",
      "Collecting pytz (from DateTime)\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: setuptools in /Users/rmian/genai/langchain/venv_langchains/lib/python3.12/site-packages (from zope.interface->DateTime) (75.1.0)\n",
      "Using cached DateTime-5.5-py3-none-any.whl (52 kB)\n",
      "Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Downloading zope.interface-7.0.3-cp312-cp312-macosx_10_9_x86_64.whl (208 kB)\n",
      "Installing collected packages: pytz, zope.interface, DateTime\n",
      "Successfully installed DateTime-5.5 pytz-2024.2 zope.interface-7.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install DateTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "id": "Xx06FAKMWmD0"
   },
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "id": "qW9SqX5jWnVW"
   },
   "outputs": [],
   "source": [
    "@tool\n",
    "def time(text: str) -> str:\n",
    "    \"\"\"Returns todays date, use this for any \\\n",
    "    questions related to knowing todays date. \\\n",
    "    The input should always be an empty string, \\\n",
    "    and this function will always return todays \\\n",
    "    date - any date mathmatics should occur \\\n",
    "    outside this function.\"\"\"\n",
    "    return str(date.today())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "id": "dTId-QjbWopi"
   },
   "outputs": [],
   "source": [
    "agent= initialize_agent(\n",
    "    tools + [time],\n",
    "    llm,\n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    handle_parsing_errors=True,\n",
    "    verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OR34lsMtWs1P",
    "outputId": "f96fbe22-bfd7-48d2-844b-f5d2be496349"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mQuestion: What's the date today?\n",
      "Thought: I can use the `time` tool to get today's date.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"time\",\n",
      "  \"action_input\": \"\"\n",
      "}\n",
      "```\n",
      "\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3m2024-10-09\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: 2024-10-09\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    result = agent(\"whats the date today?\")\n",
    "except:\n",
    "    print(\"exception on external access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
